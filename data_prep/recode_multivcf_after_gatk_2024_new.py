"""
This script takes as input a genotyped multi-sample vcf file generated by GATK(v4.4.0) GenotypeGVCFs.
For each sample, it recodes all entries with depth < {min_depth} or < 90% call support as missing data and writes the modified entries to a new output file.
It creates a separate *stats.csv file where it writes the sample name, the corresponding number of missing positions, proportion of missing data, no. of variant positions,
no. of low-depth positions and no. of 'heterozygous' positions (<90% call support) for each sample in the VCF file.
It also returns (total number of positions in vcf file, no. of sites where 'N' was mapped)

"""

import argparse
import subprocess
import os
import re
import gzip

def recode_vcf(in_file,scratch_fol,out_file,vcf_fol,min_coverage):
    bunchsize = 100000
    bunch = []
    missing_pos = {} # per sample dict for missing pos
    var_pos = {} # per sample dict for var pos
    min_AD_fail = {} # per sample dict for pos that fail depth < min_coverage
    het_pos = {} # per sample dict for pos that fail prop < 0.9 
    all_pos = 0 
    mapped_N = 0
    #uninfo_pos = 0
    #uninfo_missing_pos = {}
    
    with gzip.open(f"{str(scratch_fol)}{str(in_file)}", mode = 'rt') as file, open(f"{str(vcf_fol)}{str(out_file)}.vcf", mode = 'a+') as F, open(f"{out_file}_stats.csv", mode = 'w+') as stats:
        for line in file:
	    #print(line)
            if line.startswith("#"):        # header
                line = line[:-1]	     # "\n"
                bunch.append(line)
                if line[:6] == "#CHROM":
                    chunks = re.split('\s+',line)
                    sample_names = chunks[9:] # all samples in the vcf
                    for sample_pos in range(len(sample_names)):
                        missing_pos[sample_pos] = 0
                        var_pos[sample_pos] = 0
                        min_AD_fail[sample_pos] = 0
                        het_pos[sample_pos] = 0
                        #uninfo_missing_pos[sample_pos] = 0
                
            else:
                line = line.strip()
                chunks = re.split('\s+',line)
                sample_stats = chunks[9:]
                #print(sample_stats)
                list_temp=chunks[0:9]
                if chunks[3] == "N":
                    mapped_N += 1
                    all_pos += 1
                    for sample in sample_stats:
                        reads_count = re.split(':',sample)	# 'FORMAT' values
                        reads_count[0] = "."
                        list_temp.append(':'.join(reads_count))
                    list_t = '\t'.join(list_temp)
                    bunch.append(list_t)
                                        
                     
                else:
                    info = re.split(':', chunks[8])
                    #if 'AD' not in info:
                       # uninfo_pos += 1 # no informative reads thus 'AD' field missing
                        
                    for s in range(len(sample_stats)):                                           
                        reads_count = re.split(':',sample_stats[s])
                        #print(s,reads_count)
                        
                        if reads_count[0] == ".":
                            missing_pos[s] += 1
                        
                                                
                        elif len(reads_count)> 1:
                            info_dict = {}
                            for field_pos in range(len(reads_count)):
                                info_dict[info[field_pos]] = reads_count[field_pos]
                                #print(info_dict)
                                
                            if 'AD' in info_dict.keys(): 	# variant sites
                                alleles = re.split(',',info_dict['AD'])	# AD values
                                allele_count = [int(a) for a in alleles]
                                
                                if sum(allele_count) > 0:
                                    if int(info_dict['DP']) < sum(allele_count):	# multiallelic sites where sum of AD is not true sum and is greater than DP
                                        actual_depth = int(info_dict['DP'])
                                    else:
                                        actual_depth = sum(allele_count)
                                                             
                                    
                                    if actual_depth < int(min_coverage):
                                        #uninfo_reads += 1
                                        reads_count[0] = "."
                                        missing_pos[s] += 1
                                        min_AD_fail[s] += 1 
                                    
                                    else:
                            
                                        prop = max(allele_count)/actual_depth 
                                                                                
                                        if prop < 0.9:
                                            reads_count[0] = "."
                                            missing_pos[s] += 1
                                            het_pos[s] += 1
                                                                                         
                                        elif reads_count[0] == "0":
                                            pass
                                
                                        else:             # reads_count[0] may be 1 or 2 or 3 etc for 'true' variants
                                            var_pos[s] += 1
                                else:
                                    reads_count[0] = "."
                                    missing_pos[s] += 1
                                    min_AD_fail[s] += 1    
                                    
                            else:
                                
                                if int(info_dict['DP']) < int(min_coverage):	# DP < min depth
                                    reads_count[0] = "."	# recode GT as missing
                                    missing_pos[s] += 1
                                    #uninfo_missing_pos[s] += 1          # this is a subset of uninfo_pos    
                                    min_AD_fail[s] += 1
                        
                        
                        else: # positions that are not coded "." but still have only one info field (GT)
                            #print(list_temp,reads_count)
                            reads_count[0] = "."
                            missing_pos[s] += 1
                            
                        list_temp.append(':'.join(reads_count))                              
                                                
                    list_t = '\t'.join(list_temp)
                    bunch.append(list_t)
                    all_pos += 1

            if len(bunch) == bunchsize:
                F.write('\n'.join(bunch))
                F.write('\n')
                bunch = []
        F.write('\n'.join(bunch))
        F.write('\n')
        all_pos_without_N = int(all_pos)-int(mapped_N)
        #stats.write("Isolate,NumMissingPos,PropMissingPos,NumVariantPos,NumLowDepthPos,NumUninfoMissingPos,NumHetPos\n")
        stats.write("Isolate,NumMissingPos,PropMissingPos,NumVariantPos,NumLowDepthPos,NumHetPos\n")
        for sample_pos in range(len(sample_names)):
            stats.write(f"{sample_names[sample_pos]},{missing_pos[sample_pos]},{int(missing_pos[sample_pos])/int(all_pos_without_N)},{var_pos[sample_pos]},{min_AD_fail[sample_pos]},{het_pos[sample_pos]}\n")
    return all_pos, mapped_N
    
####parser = argparse.ArgumentParser()
####parser.add_argument('-i', '--input', metavar = '', help='name of compressed input vcf file to be recoded')
####parser.add_argument('-ip', '--input_path', metavar='', help='path to directory with input vcf file')
####parser.add_argument('-o', '--output_prefix', metavar='',help='prefix for output vcf file' )
####parser.add_argument('-op', '--output_path', metavar='',help='path to directory with output vcf file' )
####parser.add_argument('-mc','--minimum_coverage', type=int, default='', metavar='',  help='minimum coverage threshold', nargs=1)


####ag = parser.parse_args()

####if ag.input_path.strip()[-1] == "/":
    ####input_path = ag.input_path.strip()
####else:
    ####input_path = f"{ag.input_path.strip()}/"

####if ag.output_path.strip()[-1] == "/":
    ####output_path = ag.output_path.strip()
####else:
    ####output_path = f"{ag.output_path.strip()}/"
	
if __name__== '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('-i', '--input', metavar = '', help='name of compressed input vcf file to be recoded')
    parser.add_argument('-ip', '--input_path', metavar='', help='path to directory with input vcf file')
    parser.add_argument('-o', '--output_prefix', metavar='',help='prefix for output vcf file' )
    parser.add_argument('-op', '--output_path', metavar='',help='path to directory with output vcf file' )
    parser.add_argument('-mc','--minimum_coverage', type=int, default='', metavar='',  help='minimum coverage threshold', nargs=1)
    ag = parser.parse_args()

    if ag.input_path.strip()[-1] == "/":
        input_path = ag.input_path.strip()
    else:
        input_path = f"{ag.input_path.strip()}/"

    if ag.output_path.strip()[-1] == "/":
        output_path = ag.output_path.strip()
    else:
        output_path = f"{ag.output_path.strip()}/"

    myrecoded = recode_vcf(ag.input,input_path,ag.output_prefix,output_path,ag.minimum_coverage[0])
    #print(f"total no. of positions={func[0]}\nno. of N={func[1]}\nno. of sites with only uninformative reads={func[2]}")
    print(f"total no. of positions={myrecoded[0]}\nno. of N={myrecoded[1]}")
    
    
    

    



