"""
This script takes as input a genotyped multi-sample vcf file generated by GATK(v4.4.0) GenotypeGVCFs.
It recodes all genotype calls to 'diploid' calls. calls with > 90% support are treated as homozygous while those with < 90% support are heterozygous. 
For each sample, it recodes all entries with depth < {min_depth} missing data and writes the modified entries to a new output file.
It creates a separate *stats.csv file where it writes the sample name, the corresponding number of missing positions, proportion of missing data, no. of variant positions,
no. of low-depth positions and no. of 'heterozygous' positions (<90% call support) for each sample in the VCF file.
It also returns (total number of positions in vcf file, no. of sites where 'N' was mapped)

"""

import argparse
import subprocess
import os
import re
import gzip
import numpy as np

def recode_vcf(in_file,scratch_fol,out_file,vcf_fol,min_coverage):
    bunchsize = 100000
    bunch = []
    missing_pos = {} # per sample dict for missing pos
    min_AD_fail = {} # per sample dict for pos that fail depth < min_coverage
    het_pos = {} # per sample dict for pos that fail prop < 0.9
    var_pos = {} # per sample dict for variant pos
    all_pos = 0 
    mapped_N = 0
        
    with gzip.open(f"{str(scratch_fol)}{str(in_file)}", mode = 'rt') as file, open(f"{str(vcf_fol)}{str(out_file)}.vcf", mode = 'a+') as F, open(f"{out_file}_stats.csv", mode = 'w+') as stats, open(f"{out_file}_per_site_stats.csv", mode = 'a+') as per_site:
        per_site.write("site,miss,het,var,ref,total\n")
        for line in file:
	    #print(line)
            if line.startswith("#"):        # header
                line = line[:-1]	     # "\n"
                bunch.append(line)
                if line[:6] == "#CHROM":
                    chunks = re.split('\s+',line)
                    sample_names = chunks[9:] # all samples in the vcf
                    for sample_pos in range(len(sample_names)):
                        missing_pos[sample_pos] = 0
                        min_AD_fail[sample_pos] = 0
                        het_pos[sample_pos] = 0
                        var_pos[sample_pos] = 0
                
            else:
                
                miss_per_site = 0
                het_per_site = 0
                var_per_site = 0
                ref_per_site = 0
                line = line.strip()
                chunks = re.split('\s+',line)
                sample_stats = chunks[9:]
                #print(sample_stats)
                list_temp=chunks[0:9]
                if chunks[3] == "N":
                    mapped_N += 1
                    all_pos += 1
                    for sample in sample_stats:
                        reads_count = re.split(':',sample)	# 'FORMAT' values
                        reads_count[0] = "./."
                        miss_per_site += 1
                        list_temp.append(':'.join(reads_count))
                    list_t = '\t'.join(list_temp)
                    bunch.append(list_t)
                                        
                     
                else:
                    info = re.split(':', chunks[8])

                    for s in range(len(sample_stats)):                                           
                        reads_count = re.split(':',sample_stats[s])
                        #print(s,reads_count)
                        
                        if reads_count[0] == ".":
                            reads_count[0] = "./."
                            missing_pos[s] += 1
                            miss_per_site += 1
                        
                                                
                        elif len(reads_count)> 1:
                            info_dict = {}
                            for field_pos in range(len(reads_count)):
                                info_dict[info[field_pos]] = reads_count[field_pos]
                                #print(info_dict)
                                
                            if 'AD' in info_dict.keys(): 	
                                alleles = re.split(',',info_dict['AD'])	# AD values
                                allele_count = [int(a) for a in alleles]
                                
                                if sum(allele_count) > 0:
                                    if int(info_dict['DP']) < sum(allele_count):	# multiallelic sites where sum of AD is not true sum and is greater than DP
                                        actual_depth = int(info_dict['DP'])
                                    else:
                                        actual_depth = sum(allele_count)
                                                             
                                    
                                    if actual_depth < int(min_coverage):
                                        reads_count[0] = "./."
                                        missing_pos[s] += 1
                                        min_AD_fail[s] += 1 
                                        miss_per_site += 1
                                    
                                    else:
                                        
                                        if len(allele_count) == 1:
                                            reads_count[0] = "0/0"
                                            ref_per_site += 1
                                            
                                        else:
                                                                      
                                            prop = max(allele_count)/actual_depth
                                            
                                            if prop >= 0.9:
                                                allele = reads_count[0]
                                                reads_count[0] = f"{allele}/{allele}"
                                                if reads_count[0] == "0/0":
                                                    ref_per_site += 1
                                                else:
                                                    var_per_site += 1
                                                    var_pos[s] += 1
                                            
                                            else:
                                                sorted_alleles = np.argsort(allele_count)
                                                for_het_conv = np.sort(sorted_alleles[-2:])
                                                reads_count[0] = f"{for_het_conv[0]}/{for_het_conv[1]}"
                                                het_pos[s] += 1
                                                het_per_site += 1

                                else:
                                    reads_count[0] = "./."
                                    missing_pos[s] += 1
                                    min_AD_fail[s] += 1
                                    miss_per_site += 1    
                                    
                            else:
                                
                                if int(info_dict['DP']) < int(min_coverage):	# DP < min depth
                                    reads_count[0] = "./."	# recode GT as missing
                                    missing_pos[s] += 1
                                    min_AD_fail[s] += 1
                                    miss_per_site += 1
                                    
                                else:
                                    allele = reads_count[0]
                                    reads_count[0] = f"{allele}/{allele}"
                                    if reads_count[0] == "0/0":
                                        ref_per_site += 1
                                    else:
                                        print("achtung!")
                                    
                        
                        
                        else: # positions that are not coded "." but still have only one info field (GT)
                            #print(list_temp,reads_count)
                            reads_count[0] = "./."
                            missing_pos[s] += 1
                            miss_per_site += 1
                            
                        list_temp.append(':'.join(reads_count))                              
                                                
                    list_t = '\t'.join(list_temp)
                    bunch.append(list_t)
                    all_pos += 1
                
                total_per_site = miss_per_site + het_per_site + var_per_site + ref_per_site
                per_site.write(f"{all_pos + 2000},{miss_per_site},{het_per_site},{var_per_site},{ref_per_site},{total_per_site}\n")    

            if len(bunch) == bunchsize:
                F.write('\n'.join(bunch))
                F.write('\n')
                bunch = []
        F.write('\n'.join(bunch))
        F.write('\n')
        all_pos_without_N = int(all_pos)-int(mapped_N)
        
        stats.write("Isolate,NumMissingPos,PropMissingPos,NumLowDepthPos,NumHetPos,NumVarPos\n")
        for sample_pos in range(len(sample_names)):
            stats.write(f"{sample_names[sample_pos]},{missing_pos[sample_pos]},{int(missing_pos[sample_pos])/int(all_pos_without_N)},{min_AD_fail[sample_pos]},{het_pos[sample_pos]},{var_pos[sample_pos]}\n")
    return all_pos, mapped_N
    
if __name__== '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('-i', '--input', metavar = '', help='name of compressed input vcf file to be recoded')
    parser.add_argument('-ip', '--input_path', metavar='', help='path to directory with input vcf file')
    parser.add_argument('-o', '--output_prefix', metavar='',help='prefix for output vcf file' )
    parser.add_argument('-op', '--output_path', metavar='',help='path to directory with output vcf file' )
    parser.add_argument('-mc','--minimum_coverage', type=int, default='', metavar='',  help='minimum coverage threshold', nargs=1)
    ag = parser.parse_args()

    if ag.input_path.strip()[-1] == "/":
        input_path = ag.input_path.strip()
    else:
        input_path = f"{ag.input_path.strip()}/"

    if ag.output_path.strip()[-1] == "/":
        output_path = ag.output_path.strip()
    else:
        output_path = f"{ag.output_path.strip()}/"

    myrecoded = recode_vcf(ag.input,input_path,ag.output_prefix,output_path,ag.minimum_coverage[0])
    print(f"total no. of positions={myrecoded[0]}\nno. of N={myrecoded[1]}")

